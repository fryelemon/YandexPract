{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Предобработка-текста\" data-toc-modified-id=\"Предобработка-текста-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Предобработка текста</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Деление-данных\" data-toc-modified-id=\"Деление-данных-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Деление данных</a></span></li><li><span><a href=\"#Векторизация-текста\" data-toc-modified-id=\"Векторизация-текста-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Векторизация текста</a></span></li><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#LGBMClassifier\" data-toc-modified-id=\"LGBMClassifier-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>LGBMClassifier</a></span></li><li><span><a href=\"#SGDClassifier\" data-toc-modified-id=\"SGDClassifier-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>SGDClassifier</a></span></li></ul></li><li><span><a href=\"#Финальная-модель\" data-toc-modified-id=\"Финальная-модель-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Финальная модель</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Вывод</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Слава\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не знаю, чем вызвано появление первого столбца, но похоже, что это дублированный столбец с индексами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если вдруг этот столбец появится только на моей локальной машине\n",
    "try:\n",
    "    data = data.drop(['Unnamed: 0'], axis=1)\n",
    "except:\n",
    "    print('Столбца \\'Unnamed: 0\\' нет')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция будет чистить от мусора текст и приводить к единому виду\n",
    "def cleaning(text):\n",
    "    text = re.sub(r'[?(\\r|\\n)]', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто лемматизируем текст\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):    \n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 57.9 s\n",
      "Wall time: 58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         explanation why the edits made under my userna...\n",
       "1         daww he match this background colour im seemin...\n",
       "2         hey man im really not trying to edit war it ju...\n",
       "3         more i cant make any real suggestion on improv...\n",
       "4         you sir are my hero any chance you remember wh...\n",
       "                                ...                        \n",
       "159287    and for the second time of asking when your vi...\n",
       "159288    you should be ashamed of yourself that is a ho...\n",
       "159289    spitzer umm there no actual article for prosti...\n",
       "159290    and it look like it wa actually you who put on...\n",
       "159291    and i really dont think you understand i came ...\n",
       "Name: text, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data['text'] = data['text'].apply(cleaning)\n",
    "data['text'] = data['text'].apply(lemmatize)\n",
    "\n",
    "corpus = data['text']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daww he match this background colour im seemingly stuck with thanks talk january utc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данных достаточное количество, думаю имеется возможность их разделить стандартно в пропорциях `3:1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119469, 39823)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = corpus\n",
    "target = data['toxic']\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size = .25, random_state = 12345)\n",
    "\n",
    "features_train.shape[0], features_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, corpus, features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=nltk_stopwords.words('english'))\n",
    "\n",
    "tfidf_train = count_tf_idf.fit_transform(features_train)\n",
    "tfidf_test = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del count_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "{'C': 4, 'penalty': 'l1', 'random_state': 12345, 'solver': 'liblinear'}\n",
      "f1: 0.7649612906785448\n",
      "CPU times: total: 30.2 s\n",
      "Wall time: 29.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "\n",
    "param_grid = {'solver':['liblinear'],\n",
    "              'penalty' : ['l1', 'l2'],\n",
    "              'C': list(range(1,15,3)),\n",
    "              'random_state':[12345]\n",
    "             }\n",
    "\n",
    "grid_lr = GridSearchCV(model_lr, param_grid=param_grid, scoring='f1', cv=3, verbose=True)\n",
    "grid_lr.fit(tfidf_train, target_train)\n",
    "\n",
    "print(grid_lr.best_params_)\n",
    "print('f1:', grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "{'boosting_type': 'gbdt', 'learning_rate': 0.4, 'max_depth': 15, 'random_state': 12345}\n",
      "f1: 0.7490325029423879\n",
      "CPU times: total: 30min 6s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_lgbm = LGBMClassifier()\n",
    "\n",
    "param_grid = {'boosting_type': ['gbdt', 'dart', 'rf'],\n",
    "              'learning_rate': [0.1, 0.2, 0.4],\n",
    "              'max_depth': [5, 10, 15],\n",
    "              'random_state':[12345]\n",
    "             }\n",
    "\n",
    "grid_lgbm = GridSearchCV(model_lgbm, param_grid=param_grid, scoring='f1', cv=3, verbose=True)\n",
    "grid_lgbm.fit(tfidf_train, target_train)\n",
    "\n",
    "print(grid_lgbm.best_params_)\n",
    "print('f1:', grid_lgbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'random_state': 12345}\n",
      "f1: 0.7109472246245846\n",
      "CPU times: total: 1min 30s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_sgdc = SGDClassifier()\n",
    "param_grid = [{'loss':['hinge', 'log', 'modified_huber'],\n",
    "                'learning_rate':['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "                'eta0':[0.01, 0.05, 0.1, 0.2, 0.3, 0.5],\n",
    "                'random_state':[12345]\n",
    "               }]\n",
    "\n",
    "\n",
    "grid_sgdc = GridSearchCV(model_sgdc, param_grid, scoring='f1', cv=4)\n",
    "grid_sgdc.fit(tfidf_train, target_train)\n",
    "\n",
    "print(grid_sgdc.best_params_)\n",
    "print('f1:', grid_sgdc.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Финальная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат из трех проверяемых моделей показала `LogisticRegression`. Именно ее мы возьмем как финальную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7864526659412405\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(**grid_lr.best_params_)\n",
    "final_model.fit(tfidf_train, target_train)\n",
    "\n",
    "final_prediction = final_model.predict(tfidf_test)\n",
    "print(f1_score(target_test, final_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы подготовили данные для обучения:\n",
    " - Очистили и леммматизировали текст\n",
    " - Векторизировали данные\n",
    "* Мы проверили несколько моделей из которых лучшей оказалась `LogisticRegression`. \n",
    "* Проверив финальную модель на F1 метрику, мы достигли результата, который требовался."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1441,
    "start_time": "2023-06-17T13:58:49.584Z"
   },
   {
    "duration": 2338,
    "start_time": "2023-06-17T13:58:51.027Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-17T13:58:53.366Z"
   },
   {
    "duration": 33,
    "start_time": "2023-06-17T13:58:53.380Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-17T13:58:53.414Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-17T13:58:53.424Z"
   },
   {
    "duration": 3898,
    "start_time": "2023-06-17T13:58:53.430Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-17T13:58:57.329Z"
   },
   {
    "duration": 17,
    "start_time": "2023-06-17T13:58:57.334Z"
   },
   {
    "duration": 16,
    "start_time": "2023-06-17T13:58:57.353Z"
   },
   {
    "duration": 35,
    "start_time": "2023-06-17T13:58:57.370Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-17T13:58:57.407Z"
   },
   {
    "duration": 5856,
    "start_time": "2023-06-17T13:58:57.412Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-17T13:59:03.270Z"
   },
   {
    "duration": 161722,
    "start_time": "2023-06-17T13:59:03.284Z"
   },
   {
    "duration": 2101,
    "start_time": "2023-06-18T14:32:24.376Z"
   },
   {
    "duration": 3235,
    "start_time": "2023-06-18T14:32:26.479Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-18T14:32:29.716Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-18T14:32:29.731Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-18T14:32:29.747Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-18T14:32:29.757Z"
   },
   {
    "duration": 4358,
    "start_time": "2023-06-18T14:32:29.763Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-18T14:32:38.229Z"
   },
   {
    "duration": 2767,
    "start_time": "2023-06-19T13:59:41.339Z"
   },
   {
    "duration": 2878,
    "start_time": "2023-06-19T13:59:44.109Z"
   },
   {
    "duration": 17,
    "start_time": "2023-06-19T13:59:46.989Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-19T13:59:47.008Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-19T13:59:47.024Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-19T13:59:47.032Z"
   },
   {
    "duration": 5140,
    "start_time": "2023-06-19T13:59:47.054Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-19T13:59:52.196Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-19T13:59:59.214Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-19T14:00:05.469Z"
   },
   {
    "duration": 1747,
    "start_time": "2023-06-19T14:02:05.378Z"
   },
   {
    "duration": 1034,
    "start_time": "2023-06-19T14:02:07.128Z"
   },
   {
    "duration": 18,
    "start_time": "2023-06-19T14:02:08.164Z"
   },
   {
    "duration": 321,
    "start_time": "2023-06-19T14:02:08.185Z"
   },
   {
    "duration": 26,
    "start_time": "2023-06-19T14:02:08.508Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-19T14:02:08.537Z"
   },
   {
    "duration": 13564,
    "start_time": "2023-06-19T14:02:08.549Z"
   },
   {
    "duration": 16,
    "start_time": "2023-06-19T14:02:22.116Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-19T14:02:47.733Z"
   },
   {
    "duration": 1076,
    "start_time": "2023-06-19T14:02:47.739Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-19T14:02:48.816Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-19T14:02:48.825Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-19T14:02:48.843Z"
   },
   {
    "duration": 33,
    "start_time": "2023-06-19T14:02:48.858Z"
   },
   {
    "duration": 93280,
    "start_time": "2023-06-19T14:02:48.893Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-19T14:04:22.175Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-19T14:04:31.176Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
